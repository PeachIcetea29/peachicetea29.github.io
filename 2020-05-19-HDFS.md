HDFS
=====================
 범용 하드웨어로 구성된 클러스터에서 실행되고, 스트리밍 방식의 데이터 접근 패턴으로 대용량 파일을 다룰 수 있도록 설계된 파일 시스템

 ```
  + 큰 용량의 데이터 저장 가능 
     테라바이트, 페타바이트 지원

  + 스트리밍 방식의 데이터 접근
     '가장 효율적인 데이터 처리 패턴은 한번 쓰고 여러 번 읽는 것'

  + 범용 하드웨어
     고가의 신뢰도 높은 하드웨어만이 아닌, 범용 하드웨어로 구성 가능

  - 비교적 느린 응답 시간
     높은 데이터 처리량을 제공하기 위해 응답 시간 희생으로 실시간 처리에 불리

  - 부족한 메모리
     네임노드는 파일시스템의 메타데이터를 메모리에서 관리하는데, 수 많은 블록으로 인한 메타데이터로 많은 메모리가 필요
     그래서 수많은 작은 파일에 적용하기 힘듬
  
  - Single Writer
     Multi writer를 지원하지 않으므로(하둡 3.0부터는 지원) 파일 수정 작업에 어려움이 있음
 ```
<hr/>
 
Block
---------------------
* 물리적인 디스크에서 사용하는 개념으로, 특정 파일시스템에 구애받지 않고 파일을 읽고 쓸 수 있음
* HDFS도 역시 블록 개념을 가지고 있으며, 128MB를 기본 단위로 사용
    * 블록의 사이즈가 큰 이유는, 탐색 비용을 최소화
    * 블록의 시작점을 탐색하는 시간을 줄일 수 있고, 데이터를 전송하는데 더 많은 시간을 할애 할 수 있음
* HDFS 파일은 블록 크기보다 작은 데이터일 경우 디스크를 모두 점유하지 않음

## 분산 파일시스템에서 블록 개념으로 얻은 이득
* 단일 디스크의 용량보다 더 큰 파일을 분산하여 저장 가능
* 블록단위의 추상화로 스토리지의 서브시스템을 단순하게 구성할 수 있다는 것. 이는 고정크기로 용량 계산이 편하고, 메타데이터를 단순화 할 수 있음
* 내고장성<sup>fault tolerance</sup>과 가용성<sup>availability</sup>을 위한 데이터 복제 구현이 쉬움

<hr/>

Name Node & Data Node
---------------------
![](https://wikidocs.net/images/page/23582/hdfsarchitecture.png)

 HDFS Cluster는 Master-Worker 패턴으로 동작하는 네임노드<sup>namenode</sup>-데이터노드<sup>datanode</sup>로 구성되어 있다.
HDFS 클라이언트가 사용자를 대신해 네임노드와 데이터노트 사이에서 통신하고 파일시스템에 접근한다. 

- Name node
    * 파일시스템의 네임스페이스 관리
    * 파일시스템 트리와 트리에 포함된 모든 파일과 디렉터리에 대한 메타데이터를 가지고 있음
        * 메타데이터는 네임스페이스 이미지<sup>namespace image</sup>와 에디트 로그<sup>edig log</sup>라는 두 종류의 파일로 로컬 디스크에 저장
            * namespace image
                * 네임스페이스와 블록 정보
            * edit log
                * 파일의 생성, 삭제에 대한 트랜잭션 로그
                * 메모리에서 다루다가 주기적으로 저장
        * 블록의 위치 정보는 시스템이 시작할 때 모든 데이터노드로부터 받아서 재구성
    * 파일시스템의 모든 파일과 각 블록에 대한 참조 정보는 메모리에서 관리
        * 파일이 많아질 수록 확장성에서 메모리 이슈가 생김
        * 확장성 문제를 위해 HDFS federation 지원
            ```
            HDFS Federation
            * 각각의 네임노드가 파일시스템의 네임스페이스를 일부 나누어 관리하는 방식.
            * 각 네임노드는 네임스페이스의 메타데이터를 구성하는 namespace volume과 네임스페이스에 포함된 파일의 전체 블록을 보관하는 block pool을 관리한다.
            * namespace volume은 서로 독립적으로 가용성에 영향 X
            * block pool은 공유하며 사용
            * 클라이언트는 파일 경로와 네임노드를 매핑한 마운트 테이블을 이용하여 해당 HDFS fderation cluster에 접근 가능
            ```

- Data node
    * 클라이언트나 네임노드의 요청이 있을 때 블록을 저장하고 탐색
    * 주기적으로 저장하고 있는 블록의 목록을 네임노드에 보고
        * heartbeat, blockreport 
    * 자주 접근하는 블록 파일은 오프 힙<sup>off-heap</sup> 블록 캐시<sup>block cahce</sup>라는 데이터노드의 메모리에 캐싱할 수 있음
        * 사용자나 어플리케이션은 cache pool에 cache directive를 추가하여 특정 파일을 캐싱하도록 명령할 수 있음

<hr/>

장애복구를 위한 메커니즘
---------------------
 네임노드의 메타데이터를 통해 블록 정보를 알 수 있기에, 네임노드를 실행하는 머신이 손상되면 파일 시스템의 어떤 파일도 찾을 수 없다. 따라서, 네임노드의 장애 복구 기능이 필수적이다. Hadoop에서는 이를 위해 두가지 메커니즘을 제공한다.

1) 파일로 백업
    * 로컬 디스크와 원격의 NFS 마운트 두곳 동시에 백업
2) 보조 네임노드<sup>secnodary namenode</sup> 운영
    * edit log가 너무 커지지 않도록 주기적으로 네임스페이스 이미지를 edig log와 병함하여 새로운 네임스페이스 이미지를 만드는 것
    * 충분한 CPU와 메모리가 필요하므로 별도의 머신에서 실행되도록
    * 네임스페이스 이미지의 복제본 보관(단, 시간차를 두고 복제되기 때문에 어느 정도의 데이터 손실은 불가피)

네임노드의 갑작스런 장애는 거의 발생하지 않지만, 한번 장애가 발생하여 복구하는데 시간이 오래 걸릴 수 있다. 그래서 Hadoop 2.x부터는 HDFS 고가용성<sup>High availability</sup>(HA)를 지원한다.

- HA
    * active-standby 상태로 설정된 한 쌍의 네임노드로 구현
    * active namenode에 문제가 발생하면 standby namenode가 그 역할을 이어받아 중단 없이 요청을 처리하는 방식
    * 다음과 같은 HDFS 구조 변경이 필요하다.
    ```
     * 네임노드는 edit log를 공유하기 위해 고가용성 공유 storage를 필요로 함
        - 이러한 storage를 위해 NFS 필러나 QJM사용 가능
     * 데이터노드는 블록 리포트를 두개의 네임노드 모두에게 보내야 함(블록 매핑 정보는 디스크가 아닌 네임노드의 메모리에 보관되기 때문)
     * 클라이언트는 네임노드 장애를 사용자에게 투명한 방식으로 처리할 수 있도록 구성해야 함
     * standby namenode는 secondary namenode의 역할을 포함하고 있으며, active namenode의 체크포인트 작업을 주기적으로 수행해야 함
    ```

    * active namenode와 standby namenode는 모두 최신 edit log와 실시간으로 갱신되는 블록 매핑 정보를 메모리에 유지하고 있기 때문에 매우 빠르게 기존 네임노드를 대체할 수 있다.

    * standby namenode를 활성화시키는 전환 작업은 장애복구 컨트롤러<sup>failover controller</sup>로 관리된다.
        * 구현 방법은 다향하지만, 단 하나의 네임노드만 active 상태임을 보장하기 위해 주키퍼를 이용한다.
        * 각 네임노드는 가벼운 장애복구 컨트롤러 프로세스로 heartbeat 방식을 이용하여 장애를 감시하고, 장애가 발생하면 복구를 지시한다
    * 수동으로 초기화 하는 방법을 통해 우아한 장애복구<sup>graceful failover</sup>를 시도할 수 있는데, 두 네임노드가 서로 역할을 바꾸게 하는 방법으로 제어할 수 있다.
        * 장애가 발생한 네임노드가 현재 실행되지 않고 있다는 것을 확신하기 어렵기 때문에 펜싱<sup>fencing</sup>이라는 메서드를 이용한다.
        * 한 번에 다수의 네임 노드가 에디트 로그를 쓴다면 읽기 요청시 잘못된 정보를 얻을 가능성이 있기에 SSH 펜싱 명령어로 네임노드의 프로세스를 죽이도록 설정하는 것이 가장 좋은 방법
        * 공유 에디트 로그를 저장하기 위해 NFS 필러를 사용할 때는 한번에 하나의 네임노드만 허용하기 불가능하므로 더 강력한 펜싱 메서드를 사용해야 한다.
        * 네임노드가 공유 스토리지 디렉터리 접근하는 것을 막거나, 네트워크 포트를 막는 방법 등 다양한 펜싱 메커니즘이 있다.

<hr/>

하둡 파일시스템
--------------------
 * 파읽 읽기, 디렉토리 생성, 파일 이동, 데이터 삭제, 디렉토리 목록 출려과 같은 일반적인 파일시스템 연산을 모두 수행할 수 있다.
 * POSIX와 달리 파일의 실행은 지원하지 않음. 디렉토리에 대한 실행 권한은 하위 디렉토리 접근을 위해 허용
 * HTTP, C, NFS, FUSE와 같은 인터페이스를 통해 HDFS에 접근할 수 있음

<hr/>

데이터 흐름
--------------------

## 파일 읽기
![](https://www.corejavaguru.com/assets/images/content/hdfs-data-flow-read.png)

- 설계 
    * 클라이언트는 데이터를 얻기 위해 데이터노드에 직접적으로 접촉하며, 네임노드는 각 블록에 적합한 데이터노드를 안내해준다.
    * 데이터 트래픽을 클러스터에 있는 모든 데이터노드에 고르게 분산시켜, 동시에 실행되는 클라이언트의 수를 늘릴 수 있다.
    * 네임노드의 경우, 메타데이터를 메모리에 저장하고 단순히 블록의 위치 정보 요청만 처리하기 때문에 클라이언트가 많아져도 병목현상이 거의 발생하지 않는다.

- 흐름
    ```
    1. 클라이언트는 open 메서드를 호출하여 원하는 파일을 연다

    2. DistributedFileSystem은 파일의 첫 번째 블록 위치를 파악하기 위해 RPC를 사용하여 네임노드를 호출, 네임노드로부터 해당 블록의 복제본을 가진 데이터노드의 주소를 얻음(이때, 클라이언트와 가까운 순으로 데이터 노드가 정렬된다) DistributedFileSystem은 클라이언트가 데이터를 읽을 수 있도록 FSDataInputStream을 반환하고, FSDataInputStream은 데이터노드와 네임노드의 I/O를 관리하는 DFSInputStream을 래핑한다.

    3. 클라이언트는 Stream를 읽기 위해 read 메서드를 호출한다. DFSInputTream은 가장 가까운 데이터노드와 연결한다

    4. 해당 stream에 대해 read 메서드를 반복 호출하며 데이터노드에서 클라이언트로 모든 데이터를 받는다.

    5. 블록의 끝에 도달하면 DFSInputstream은 데이터노드의 연결을 닫고, 다음 블록의 데이터노드를 찾는다. 클라이언트 관점에서는 이러한 과정이 단지 연속적인 스트림을 읽는 것처럼 느껴진다.

    6. 모든 블록에 대한 read가 끝나면, 클라이언트는 FSDataInputStream의 close 메서드를 호출한다
    ```
- error
    + 데이터를 읽는 중에 데이터노드와의 통신 장애가 발생하면, 다른 데이터 노드와 연결을 시도한다.
    + 불필요한 재시도를 줄이기 위해 장애가 발생한 데이터노드를 기억해둔다.
    + 데이터노드로부터 전송된 checksum 검증도 한다
    + 블록이 손상된 경우, 다른 블록의 복제본을 읽으려 시도한다. 마찬가지로 손상된 블록에 대한 정보는 네임노드에 보고된다.

+ 가장 가까운 데이터 노드?
    1. 동일 노드
    2. 동일 rack의 다른 노드
    3.  동일 데이터 센터에 있는 다른 rack의 노드
    4.  다른 데이터 센터에 있는 노드


## 파일 쓰기

![](https://www.corejavaguru.com/assets/images/content/hdfs-data-flow-read.png)

- 흐름
    ```
    1. 클라이언트는 create 메서드를 호출하여 파일을 생성

    2. DistributedFileSystem은 파일시스템의 네임스페이스에 새로운 파일을 생성하기 위해 네임노드에 RPC 요청. 이때 블록에 대한 정보는 보내지 않으며, 네임노드는 요청한 파일과 동일 파일이 이미 존재하는지, 클라이언트가 파일을 생성할 권한을 가지고 있는지 등 다양한 검사를 수행한다. 검사를 통과하지 못하면 실패하며 IOException이 발생하고, 통과한 경우 클라이언트에 FSDataOutputStream을 반환한다. 읽을 때와 마찬가지로 DFSOutputStream으로 래핑된다.

    3. 클라이언트가 데이터를 쓸 때 DFSOutputStream은 데이터를 패킷으로 분리하고, data queue라 불리는 내부 큐로 패킷을 전송한다. 
    
    4. 네임노드에 복제본을 저장할 데이터노드의 목록을 요청하고, 파이프라인을 형성하여 패킷을 각각의 데이터노드에 순서대로 전송한다.

    5. 데이터노드의 승인 여부를 기다리는 ack queue라 불리는 내부 패킷 queue를 유지하는데, ack queue에 있는 패킷은 파이프라인의 모든 데이터노드로부터 ack 응답을 받아야 제거된다.

    6. 모든 블록에 대한 데이터 쓰기가 완료되면, 클라이언트는 FSDataInputStream의 close 메서드를 호출한다.

    7. 모든 패킷이 완전히 전송되면 네임노드에 파일 완료 신호를 보낸다.
    ```
- error
    + 데이터를 쓰는 중에 데이터노드에 장애가 발상하면, 파이프라인이 닫히고 ack queue에 있는 모든 패킷들이 다시 data queue로 추가된다.
    + 데이터노드는 네임노드로부터 새로운 ID를 다시받고, 장애가 발생한 데이터노드가 다시 복구되면 불완전한 블록은 삭제된다.
    + 장애 데이터노드는 파이프라인에서 제거되며, 나머지 데이터노드로 파이프라인을 구성한다.
    + 네임노드는 해당 블록이 불완전 복제라는 것을 인식하고 있으므로 나중에 다른 노드에 복제본이 생성되도록 조치한다.

+ 복제본 배치 전략
    + 첫 번째 복제본을 클라이언트와 같은 노드에 배치(클라이언트가 클러스터 외부에 있으면 무작위로 노드를 선택)
    + 두 번째 복제본은 첫 번째 노드와 다른 랙(무작위)의 노드에 배치
    + 세 번째 복제본은 두 번째 노드와 같은 랙의 다른 노드에 배치
    + 그 이상의 복제본은 클러스터에서 무작위로 배치(같은 랙에 배치되지 않도록 설계)

