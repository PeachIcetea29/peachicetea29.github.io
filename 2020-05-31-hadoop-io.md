Hadoop I/O
==================

* Checksum을 이용해 손상된 데이터 검출
* HDFS에서는 CRC-32C라고 불리는 변형 체크섬을 사용한다

## HDFS의 데이터 무결성

* HDFS는 모든 데이터를 쓰는 과정에서 내부적으로 체크섬을 계산, 읽는 과정에서 검증한다.
* 데이터노드는 데이터와 체크섬을 저장하기 전에 수신한 데이터를 검증
* 파이프라인의 마지막 데이터노드가 해당 데이터의 체크섬을 검증
* 클라이언트가 데이터노드로부터 데이터를 읽을 때 클라이언트 역시 데이터노드에 저장된 체크섬과 수신된 데이터로부터 체크섬을 검증
* 체크섬에 대한 로그를 저장해서 오류 검출에 유용
* 각 데이터노드는 저장된 모든 블록을 주기적으로 검증하는 DataBlockScanner를 백그라운드 스레드로 수행. 물리 저장 매체에서 발생할 수도 있는 비트 로트<sup>bit rot</sup>에 대한 데이터 손실을 피함
 * 체크섬을 이용해 손상된 복제본 복구

 ## LocalFileSystem

* 클라이언트 측 체크섬을 수행
* 기존 파일시스템이 자체적으로 체크섬을 지원한다면 사용하지 않아도 괜찮음
* ChecksumFileSystem을 활용한다.

## 압축
* 파일 용량을 줄이고, 네트워크 또는 디스크로부터 전송 속도를 높일 수 있음
* 모든 압축 알고리즘은 압축과 해제가 빨라질수록 공간이 늘어나는 희생을 감수해야 하기 때문에 공간과 시간은 트레이드오프 관계에 있다.
* 맵리듀스에는 분할 가능한 압축 포맷이 적합하다

## 코덱

* 압축-해제 알고리즘을 구현한 것
* CompressionCodec을 이용해 대이터를 압축하거나 해제
    * 출력 스트림에 쓸 데이터를 압축할때는 createOutputStream 메서드
    * 입력 스트림으로부터 읽어 들인 데이터를 압축 해제하려면 createInputStream 메서드
    * CompressionCodecFactory를 사용하여 어떤 codec인지 유추 가능

* 성능 관점에서는 압축과 해제를 위해 원시 라이브러리를 사용하는 것이 바람직
    * 원시 gzip 라이브러리를 사용했을 때, 압축 해제 성능은 최대 50%, 압축 성능은 거의 10%정도 더 좋아짐(내장된 자바 구현과 비교)
* 기본적으로 하둡은 자신이 수행되는 플랫폼에 맞는 원시 라이브러리를 먼저 찾고, 있으면 자동으로 해당 라이브러리를 로드한다
* 원시 라이브러리를 사용하고 application에서 상당히 많은 압축 또는 해제 작업을 수행해야 한다면, CodecPool을 사용하는 것이 좋다

```
                                        어떤 압축 포맷을 사용하는게 좋을까?

1) 압축과 분할 모두 지원하는 시퀀스 파일, 에이브로, ORCFile, 파케이 같은 컨테이너 파일 포맷
2) bzip2 같이 분할 지원하는 압축 포맷. 분할을 지원하기 위해 색인될 수 있는 LZO같은 포맷
3) 애플리케이션에서 파일을 청크로 분할하고, 지원되는 모든 압축 포맷을 사용해 각 청크를 개별적으로 압축(압축된 청크 크기가 하나의 HDFS 블록 만큼의 크기가 되도록)
4) 파일을 압축하지 말고 그냥 저장
```

## 직렬화

* 네트워크 전송을 위해 구조화된 객체를 바이트 스트림으로 전환하는 과정
* 하둡 시스템에서 RPC를 이용해 노드 사이의 프로세스간 통신이 이루어지는데, 이 때 메시지를 하나의 바이너리 스트림으로 구성하기 위해 직렬화를 사용
* RPC뿐만아니라 영속적 데이터 저장에도 좋음
* 장점
    * 간결성
        * 네트워크 대역폭 절약
    * 고속화
        * 오버헤드를 줄여 backbone 형성에 용이
    * 확장성
        * 새로운 요구사항을 만족시키기 위한 프로토콜의 발전에 용이
    * 상호운용성
        * 다양한 언어로 작성된 클라이언트 지원 가능

## Writable
* 하둡에서는 Writable이라는 직렬화 포맷 사용
* org.apache.hadoop.io 패키지에서 여러 Writable 클래스 제공


## 자바 기본 자료형 Writable wrapper
* char를 제외한 모든 자바 기본 자료형을 위한 writable wrapper 존재
* get과 set메서드를 이용해 값 이용

|자바 기본 자료형 | Writable 구현체 | 직렬화된 크기(바이트)|
|--------------  |----------------|------|
|boolean|BooleanWritable|1|
|byte|ByteWritable|1|
|short|ShortWritable|2|
|int|IntWritable(고정길이)|4|
|   |VIntWritable(가변길이)|1~5|
|float|FloatWritable|4|
|long|LongWritable(고정길이)|8|
|   |VLongWritable(가변길이)|1~9|
|double|DoubleWritable|8|

* 숫자값은 대부분 균일하지 않게 분포되어 가변길이 인코딩을 선택하는 것이 저장 공간 절약면에서 좋음

## Text
* UTF-8 시퀀스
* 가변길이 인코딩으로 int를 사용
* Text 클래스에서 인덱스는 인코딩된 바이트 시퀀스의 위치 관점(string처럼 자바 char 코드 단위 위치 관점이 아님)
* Text에서 유니코드 문자를 반복할 때는 인덱스를 위해 바이트 오프셋을 사용해야 하므로 복잡한 편
* String과의 다른 차이점은 가변적이라는 것
* tostring()을 이용해 string으로 변환 가능

## BytesWritable
* 바이너리 데이터의 배열에 대한 wrapper
* 데이터의 바이트 길이를 지정하는 4바이트 정수 필드에 해당 데이터가 뒤따르는 구조
    * ex) 3과 5 값을 가지는 길이가 2인 바이트 배열 => 4바이트 정수(00000002) + 2바이트(03) + 2바이트(05) = 000000020305
* 가변적이고 set()을 이용해 변경 가능

## NullWritable
* 길이가 0인 특별한 직렬화. 어떠한 바이트도 읽거나 쓸 수 없다
* placeholder로 사용
* singleton이기에, get()으로 호출하여 얻어야 한다.

## ObjectWritable & GenericWritable
* ObjectWritable은 자바 기본 자료형 및 자료형 배열을 위한 범용 wrapper
* RPC에서 메서드 parameter와 return type을 위해 사용
* 범용 메커니즘을 직렬화할 때마다 쓰는 것은 공간 낭비가 생길 수 있기 때문에, 자료형의 수가 적고 미리 알려진 경우에는 정적 자료형 배열과 그 자료형에 대한 직렬화된 배열의 인덱스를 사용하는 GenericWritable로 공간 낭비를 줄일 수 있다.

## Writable collection
* collection type 지원
    * Array, ArrayPrimitve, TwoDArray, Map, SortedMap, EnumSet 등 
