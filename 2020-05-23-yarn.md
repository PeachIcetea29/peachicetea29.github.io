YARN
=====================

* 하둡의 클러스터 자원 관리 시스템
* 클러스터의 자원을 요청하고 사용하기 위한 API 제공
* 사용자 코드에서 이 YARN API를 직접 사용하는 것이 아니라, YARN이 내장된 분산 컴퓨팅 프레임워크에서 고수준 API를 작성해서 YARN의 API를 사용하는 것

![](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F272AE93F58BFC09824)
<hr/>

## YARN의 Application 수행
![](https://imgs.developpaper.com/imgs/2774863665-5d7b670e13da1_articlex.png)
* 클러스터 전체 자원의 사용량을 관리하는 리소스 매니저
* 컨테이너를 구동하고 모니터링 하는 노드 매니저

### 수행 과정
```
 1) 클라이언트는 YARN에서 application을 구동하기 위해 리소스 매니저에 접속하여 application master process 구동 요청
 2) 리소스 매니저는 application master를 구동할 수 있는 노드 매니저를 찾는다.
 3) application에 따라 단일 컨테이너에서 계산을 수행하고 클라이언트에 그 결과를 반환한다. 리소스 매니저에 더 많은 컨테이너를 요청한 후 분산처리를 수행하는 경우도 있다.
```

## Application 수명
* 사용자의 job당 하나의 application이 실행되는 방식 ex) Map-Reduce Job
* work flow 혹은 사용자의 job session 당 하나의 application이 실행되는 방식 ex) spark
    * 순차적으로 실행되는 job이 동일한 컨테이너를 재사용 할 수 있다.
    * job 사이에 공유 데이터를 캐싱할 수 있다
* 서로 다른 사용자들이 공유할 수 있는 장기 실행 application ex) impala
    * 다양한 application을 구동시키는 applcation master가 있다
    * impala의 경우 proxy application을 이용해 여러 impala 데몬이 클러스터 자원을 요청할 수 있도록 한다

## application 작성
* Job의 DAG<sup>directed acyclic graph</sup>을 실행하고 싶다면 스파크 or 테즈
* 스트리밍 처리는 스파크, 쌈자<sup>samza</sup> 또는 스톰<sup>storm</sup>을 추천
* 아파치 슬라이더는 기존 분산 어플리케이션을 YARN위에서 실행하도록 해줌
    * 여러 사용자가 동일한 application의 서로 다른 버전을 실행할 수 있음
    * 실행되는 노드의 수를 변경하거나, 실행 application을 중지하고 다시 시작하게 제어할 수 있음
* 아파치 트윌은 슬라이더와 비슷하지만 YARN에서 실행되는 분산 application을 개발할 수 있는 프로그래밍 모델 제공
    * 실시간 로깅과 명령 메시지 기능 제공

<hr/>

YARN과 Map Reduce 1(하둡 구버전) 차이
--------------------
## Map Reduce1
* Map reduce 1의 경우, Job의 실행 과정을 제어하는 하나의 Jobtracker와 하나 이상의 Tasktracker 두 종류의 데몬으로 이루어져있다.
* JobTracker는 여러 tasktracker에서 실행되는 task를 스케줄링하여 시스템에서 실행되는 모든 job을 조율한다. 각 job의 전체적인 상황을 파악하며, task가 실패하면 다른 tasktracker에 실패한 task를 다시 스케줄링한다.
* TaskTracker는 task를 실행하고 진행 상황을 jobtracker에 전송한다.

## YARN
* Map Reduce1과 달리 역할을 분리하여 리소스 매니저와 application 마스터(Job당 하나)를 통해 처리한다.
* Map Reduce1의 여러 한계를 극복할 수 있다
    * 확장성
        - Map reduce1보다 큰 클러스터에서 실행될 수 있다. JobTracker가 Job과 Task모두를 관리하는 Map reduce1과는 달리, 리소스 매니저와 application 마스터로 분리하기 때문
    * 가용성
        - 서비스 데몬에 문제가 발생했을 때, 서비스에 필요한 작업을 다른 데몬이 이어받을 수 있도록 상태 정보를 항상 복사해두는 방법으로 구현되는게 일반적. 하지만 이는 각 task의 상태가 수 초마다 변경되기 때문에, 상태 정보가 매우 빠르게 변경되는 상황에서 JobTracker에 HA를 적용하는 것은 힘들다.
        - JobTracker의 역할이 리소스 매니저와 application 마스터로 분리되었기 때문에 HA는 divide and conquer문제로 바뀌었다. 즉, 리소스 매니저의 HA를 제공한 후 YARN application 마스터 모두에 HA를 제공한다.
    * 효율성
        - Map Reduce1에서는 고정된 크기의 '슬롯' 정적 할당으로 이용한다.
        - YARN은 슬롯 대신 리소스 풀을 관리하여, 클러스터의 맵 슬롯은 남아 있지만 리듀스 슬롯이 없어서 리듀스 태스크가 대기하는 상황이 발생하지 않는다.
        - 기존에는 슬롯의 사용으로 자원의 낭비나, 너무 적게 자원을 할당하는 경우가 존재했다. YARN의 자원은 잘게 쪼개져 있기 때문에, application은 필요한 만큼만 자원을 요청할 수 있다.
    * 다중 사용자
        - 다양한 분산 application을 수용할 수 있다. Map reduce는 YARN의 application 중 하나일 뿐

<hr/>

YARN Scheduling
--------------------

## FIFO
![](https://www.corejavaguru.com/assets/images/content/bigdata/yarn-fifo-scheduler.png)
* application을 queue에 하나씩 넣고 제출된 순서에 따라 순차적으로 실행
* 이해하기 쉽고 설정이 필요 없다는 장점이 있지만, 공유 클러스터 환경에서는 적합하지 않다(자기 차례가 올 때까지 계속 대기해야 하기 때문)

## Capacity
![](https://lh3.googleusercontent.com/proxy/e1vNCpOLufqZCI-72Ksk7vQDatpVizw8ZTNd9ZUiU9zcZnSYYidRCHKDzeo3cVB_z6LlEnfYp1MBUuXLmx-UMSlnPGY67gY1MoGOD2QVECFKdQL244uTZsib8jYNeHVUf7I9jemUGQ7YF8IyQtxXsymyBWBWVoH1wK-k82G-xD1ssP9MrqEcq0Pao6oN6xFUB4lGBzvuYr-TYfnfn8zyhqNArlOqKtOu5hsizhJp1pMvjEkwg1XijvleK8C0uEpB066NN_2zsVDPX_lQXZXF9y3y4TcWmN5ojxRCPoypPgvUnA)
* 특징 
    * 분리된 전용 queue에서 Job을 처리
    * 분리된 queue는 자원을 미리 선점하기 때문에 전체 클러스터의 효율성은 떨어지며, 대형 JOB의 경우 FIFO보다 늦게 끝날 수 있다
    * 회사의 조직 체계에 맞게 하둡 클러스터를 공유할 수 있다. 각 조직은 분리된 전용 queue를 가지며 클러스터 가용량의 지정된 부분을 사용할 수 있도록 설정할 수 있다.
    * 분리된 queue 내부에서는 FIFO 방식으로 스케줄링된다.
* queue 탄력성<sup>queue elasticity</sup>
    * 단일 Job은 해당 queue의 가용량을 넘는 자원을 사용할 수 없다.
    * 하지만 queue 안에 다수의 Job이 존재하고, 자원이 클러스터에 남아있다면 해당 queue에 있는 job을 위해 여분의 자원을 할당할 수 있다.
    * queue의 가용량을 초과한 자원을 할당하게 되는데, 이를 queue 탄력성이라 한다.
* 설정
    * 계층형 구조로 queue를 직접 설정할 수 있다.
    * 가용량과 application에 할당될 자원의 최대 개수, 동시에 실행되는 application의 개수, queue의 접근 제어 목록(ACL)등을 제어하는 설정도 가능하다.
* queue 배치
    * application의 종류에 따라 queue에 배치할 수 있다.
    * queue를 지정하지 않으면 기본 queue인 default에 배치된다.
 

## Fair
![](https://www.corejavaguru.com/assets/images/content/bigdata/yarn-fair-scheduler.png)
* 특징
    * 모든 Job의 자원을 동적으로 분배한다.
    * Job이 실행되는 도중 다른 Job이 시작되면 페어 스케줄러는 클러스터 자원을 공평하게 절반을 할당한다.
        * queue끼리만 공평하게 자원을 분배하고, queue 내부에서는 자원을 다르게 분배할 수 있다.
    * 클러스터의 효율성도 높고 작은 Job도 빨리 처리되는 효과가 있다.
* 활성화  
    * 보통 기본 스케줄러는 capacity 스케줄러이기 때문에 fair 스케줄러를 활성화 하려면 yarnsite.xml파일의 yarn.resourcemanager.scheduler.class 속성에 스케줄러의 전체 클래스 이름을 org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler와 같이 지정해야 한다.
* 설정
    * 클래스 경로에 있는 fair-scheduler.xml이라는 할당 파일에 원하는 속성을 설정한다.
    * 할당 파일이 없으면 fair 스케줄러는 그림과 같이 작동한다(각 application이 해당 사용자 이름의 queue에 배치)
    * 사용자 queue는 해당 사용자가 처음 application을 제출할 때 동적으로 생성된다.
    * 계층 구조 가능
    * 각 queue에 비율로 적용되는 가중치를 설정하여 다르게 자원을 할당할 수 있다 ex) 4:6 
        * 가중치를 설정하지 않으면 각 queue는 기본으로 1의 가중치를 가진다
    * 각 queue에 FIFO, fair, DRF등 서로 다른 스케줄링을 적용할 수 있다
    * 각 queue에 최소와 최대 자원 사용량과 최대 실행 application의 개수도 지정할 수 있다.
        * 최소 자원 설정은 preemption에도 활용할 수 있다.
* queue 배치
    * 규칙에 따라 application을 배치하게 되는데, 맞는 규칙이 나올때까지 순서대로 시도한다.
    * specified는 지정된 queue에 application을 배치
    * primaryGroup은 사용자의 유닉스 그룹의 이름을 가진 queue에 배치
    * default는 항상 해당 명시한 queue에 배치
    * queue를 명시적으로 지정하지 않으면 사용자 이름의 queue를 사용한다. queue가 없는 경우에는 자동으로 생성 
    * yarn.scheduler.fair.user-as-default-queue 속성을 false로 설정하면 default queue에 모든 application이 배치된다.
    * 동적으로 queue 생성을 막는 설정도 가능하다.
* 선점<sup>preemption</sup>
    * yarn.scheduler.fair.preemption 속성을 true로 설정하여 활성화
    * 아래 두가지 타임아웃 설정중 반드시 하나는 지정해야 함
    * 최소 공유 선점 타임아웃<sup>minimum share preemption timeout</sup>
        * queue가 최소 보장 자원을 받지 못한채 해당 timeout이 발생하면 다른 컨테이너를 선점할 수 있다.
    * 균등 공유 선점 타임아웃<sup>fair share preemption timeout</sup>
        * queue가 균등 공유의 절반 이하인 상태에서 해당 timeout이 발생하면 다른 컨테이너를 선점할 수 있다.

## 지연 스케줄링
* 조금 기다리면 요청한 지정 노드에서 컨테이너를 할당받을 수 있는 기회가 급격하게 증가한다는 사실을 이용
* 클러스터의 효율성을 높일수 있다.
* heartbeat를 통해 컨테이너의 정보와 컨테이너를 위한 자원에 대한 정보를 주고받게 되는데, 이를 이용해 스케줄링 기회를 얻는다.
* 처음 오는 스케줄링 기회를 바로 사용하지 않고, 스케줄링 기회의 최대 횟수까지 기다린 후 다음에 오는 기회를 잡는다.

## 우성 자원 공평성<sup>Dominant Resource Fairness</sup>(DRF)
* 각 사용자의 우세한 자원(메모리, CPU 등)을 확인한 후, 이를 클러스터 사용량의 측정 기준으로 삼는 것
* 기본적으로 비활성화 되어있다.
```
예시)
 Cluster => 100 CPU, 10TB memory

 Application A => 2 CPU, 300GB memory needs
 Application B => 6 CPU, 100GB memory needs

 A request => CPU needs(2%) memory needs(3%)
 B request => CPU needs(6%) memory needs(1%)

 각 우세 자원기준으로 비교해 보면 A(memory-3%)보다 B(CPU-6%)가 2배 높기 때문에, B는 A의 두배에 해당하는 자원을 할당받게 된다. 

```